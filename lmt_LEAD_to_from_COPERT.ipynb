{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and install required packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the applicable line to install (needed only once)\n",
    "\n",
    "# For MacOs and Linux\n",
    "# %pip install tk\n",
    "# %pip install openpyxl\n",
    "# %pip install pandas\n",
    "# %pip install requests\n",
    "\n",
    "# For Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requiered libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tkinter\n",
    "import csv\n",
    "from csv import reader\n",
    "import os\n",
    "import json\n",
    "from tkinter import filedialog\n",
    "from tkinter import *\n",
    "import pandas as pd\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_button():\n",
    "    # select a directory and store it in global var \"folder_path\"\n",
    "    global folder_path\n",
    "    filename = filedialog.askdirectory(initialdir='/Users/angel/Dropbox (Last Mile Team)/Mac/Desktop')\n",
    "    folder_path.set(filename)\n",
    "    print(filename)\n",
    "    root.destroy()\n",
    "\n",
    "root = Tk()\n",
    "folder_path = StringVar()\n",
    "lbl1 = Label(master=root,textvariable=folder_path)\n",
    "lbl1.grid(row=0, column=1)\n",
    "button2 = Button(text=\"Browse\", command=browse_button)\n",
    "button2.grid(row=0, column=3)\n",
    "\n",
    "mainloop()\n",
    "directory = os.path.join(folder_path.get())\n",
    "os.chdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from lmt_LEAD_input_to_COPERT.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"lmt_LEAD_input_to_COPERT.xlsx\", \"URBAN_OFF_PEAK_SPEED\")\n",
    "urban_off_peak_speed = df.iloc[:,4]\n",
    "df2 = pd.read_excel(\"lmt_LEAD_input_to_COPERT.xlsx\", \"URBAN_PEAK_SPEED\")\n",
    "urban_peak_speed = df2.iloc[:,4]\n",
    "df3 = pd.read_excel(\"lmt_LEAD_input_to_COPERT.xlsx\", \"URBAN_PEAK_SHARE\")\n",
    "urban_peak_share = df3.iloc[:,4]\n",
    "df4 = pd.read_excel(\"lmt_LEAD_input_to_COPERT.xlsx\", \"URBAN_OFF_PEAK_SHARE\")\n",
    "urban_off_peak_share = df4.iloc[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete rows 2 to 12 of selected lmt_LEAD_input_to_COPERT.xlsx worksheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    " \n",
    "# load excel file\n",
    "workbook = load_workbook(filename=\"lmt_LEAD_input_to_COPERT.xlsx\")\n",
    " \n",
    "# open workbook\n",
    "sheet = workbook.active\n",
    "\n",
    "sheet_names = [\"URBAN_OFF_PEAK_SPEED\", \"URBAN_PEAK_SPEED\", \"URBAN_OFF_PEAK_SHARE\", \"URBAN_PEAK_SHARE\", \"STOCK\", \"MEAN_ACTIVITY\"]\n",
    "for sheet in sheet_names:\n",
    "    workbook[sheet].delete_rows(2, 12)\n",
    "workbook.save(filename = \"lmt_LEAD_input_to_COPERT.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to lmt_LEAD_input_to_EVCO2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import json\n",
    "l = directory.split(\"_\")\n",
    "if 'diesel' in l:\n",
    "    category = 'Light Commercial Vehicles'\n",
    "    fuel = 'Diesel'\n",
    "    segment = 'N1-III'\n",
    "    num = 161\n",
    "    eurostandard = 'Euro 6 a/b/c'\n",
    "elif 'hybrid' in l:\n",
    "    category = 'Passenger Cars'\n",
    "    fuel = 'Petrol Hybrid'\n",
    "    segment = 'Large-SUV-Executive'\n",
    "    eurostandard = 'Euro 6 a/b/c'\n",
    "    num = 161\n",
    "elif 'nv200' in l:\n",
    "    category = 'HeavyCommercial Vehicles'\n",
    "    fuel = 'Diesel'\n",
    "    segment = 'N1-III'\n",
    "    eurostandard = 'Euro 6 a/b/c'\n",
    "    num = 100\n",
    "for path,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if \"lmt_LEAD_input_to_EVCO2_2.json\" in file or \"lmt_LEAD_input_to_EVCO_2.json\" in file:\n",
    "            dd = pd.read_json(\"lmt_LEAD_input_to_COPERT.json\")\n",
    "            dd2 = pd.read_json(\"lmt_LEAD_input_to_EVCO2_2.json\")\n",
    "            dd['ResponsePlanId'] = dd2['ResponsePlanId']\n",
    "            dd['Category'] = category\n",
    "            dd['Fuel'] = fuel\n",
    "            dd['Segment'] = segment\n",
    "            dd['EuroStandard'] = eurostandard\n",
    "            for path,dirs,files in os.walk(directory):\n",
    "                for file in files:\n",
    "                    if file.endswith(\".xml\") and 'Response' in path:\n",
    "                        with open(os.path.join(path, file), 'r') as f:\n",
    "                            a = f.read()\n",
    "                            f.close()\n",
    "            xml_pattern = \"(?:<NumberOfServices.*?>)(.*?)(?:<\\\\/NumberOfServices>)\"\n",
    "            val = re.findall(xml_pattern, a)[0]\n",
    "            dd['Stock'] = math.ceil(int(val) / num)\n",
    "            dd['MeanActivity'] = 60\n",
    "            with open(\"lmt_LEAD_input_to_COPERT.json\", \"w\") as ff:\n",
    "                data = json.loads(dd.to_json(orient = \"records\"))\n",
    "                ff.truncate()\n",
    "                ff.write(json.dumps(data))\n",
    "                ff.close()\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to lmt_LEAD_input_to_COPERT.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_json(\"lmt_LEAD_input_to_COPERT.json\")\n",
    "d = d.rename(columns = {\"Stock\":\"2021\", \"MeanActivity\":\"2021_\"})\n",
    "d2 = d[[\"Category\", \"Fuel\", \"Segment\", \"EuroStandard\"]]\n",
    "d3 =  d[[\"Category\", \"Fuel\", \"Segment\", \"EuroStandard\",\"2021\"]]\n",
    "d4 = d[[\"Category\", \"Fuel\", \"Segment\", \"EuroStandard\",\"2021_\"]]\n",
    "d4 = d4.rename(columns = {\"2021_\":\"2021\"})\n",
    "if len(urban_peak_speed) != d2.shape[0]:\n",
    "    if len(urban_peak_speed) > d2.shape[0]:\n",
    "        urban_peak_speed = urban_peak_speed[:d2.shape[0]]\n",
    "        urban_off_peak_speed = urban_off_peak_speed[:d2.shape[0]]\n",
    "        urban_peak_share = urban_peak_share[:d2.shape[0]]\n",
    "        urban_off_peak_share = urban_off_peak_share[:d2.shape[0]]\n",
    "    else:\n",
    "        while len(urban_peak_speed) != d2.shape[0]:\n",
    "            urban_peak_speed =   urban_peak_speed.append(pd.Series(urban_peak_speed[0]), ignore_index = True)\n",
    "            urban_off_peak_speed = urban_off_peak_speed.append(pd.Series(urban_off_peak_speed[0]), ignore_index = True)\n",
    "            urban_peak_share = urban_peak_share.append(pd.Series(urban_peak_share[0]), ignore_index = True)\n",
    "            urban_off_peak_share = urban_off_peak_share.append(pd.Series(urban_off_peak_share[0]), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from lmt_LEAD_input_to_COPERT.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheets = [\"URBAN_OFF_PEAK_SPEED\", \"URBAN_PEAK_SPEED\", \"URBAN_OFF_PEAK_SHARE\", \"URBAN_PEAK_SHARE\"]\n",
    "book = load_workbook('lmt_LEAD_input_to_COPERT.xlsx')\n",
    "writer = pd.ExcelWriter('lmt_LEAD_input_to_COPERT.xlsx', engine='openpyxl')\n",
    "writer.book = book\n",
    "writer.sheets = {ws.title: ws for ws in book.worksheets}\n",
    "for sheetname in writer.sheets:\n",
    "    if sheetname == \"URBAN_OFF_PEAK_SPEED\":\n",
    "        d2[df.columns[4]] = urban_off_peak_speed.values\n",
    "        d2.to_excel(writer,sheet_name=sheetname, startrow=0, index = False)\n",
    "    elif sheetname == \"URBAN_PEAK_SPEED\":\n",
    "        d2[df2.columns[4]] = urban_peak_speed.values\n",
    "        d2.to_excel(writer,sheet_name=sheetname, startrow=0, index = False)\n",
    "    elif sheetname == \"URBAN_OFF_PEAK_SHARE\":\n",
    "        d2[df3.columns[4]] = urban_off_peak_share.values\n",
    "        d2.to_excel(writer,sheet_name=sheetname, startrow=0, index = False)\n",
    "    elif sheetname == \"URBAN_PEAK_SHARE\":\n",
    "        d2[df4.columns[4]] = urban_peak_share.values\n",
    "        d2.to_excel(writer,sheet_name=sheetname, startrow=0, index = False)\n",
    "    elif sheetname == \"STOCK\":\n",
    "        d3.to_excel(writer,sheet_name=sheetname, startrow=0, index = False)\n",
    "    elif sheetname == \"MEAN_ACTIVITY\":\n",
    "        d4.to_excel(writer,sheet_name=sheetname, startrow=0, index = False)\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send lmt_LEAD_input_to_COPERT.xlsx to LEAD COPERT VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {\n",
    "    'input_data': open(directory + '/lmt_LEAD_input_to_COPERT.xlsx', 'rb'),\n",
    "    'keep ': (None, 'true'),\n",
    "    'noevap': (None, 'false'),\n",
    "    'alt_country ': (None, 'ES'),\n",
    "    'EF': (None, 'true'),\n",
    "    'EB': (None, 'true'),\n",
    "    'outputFile': (None, 'xlsx'),\n",
    "}\n",
    "\n",
    "response = requests.post('http://3.120.144.230:8000', files=files)\n",
    "\n",
    "with open(directory + '/Response_from_COPERT.zip', 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download, store and clean LEAD COPERT VM response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(directory + \"/Response_from_COPERT\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(directory + '/Response_from_COPERT.zip', 'r') as zip:\n",
    "    zip.extractall(directory + \"/Response_from_COPERT/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(directory + '/Response_from_COPERT.zip'):\n",
    "    os.remove(directory + '/Response_from_COPERT.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
